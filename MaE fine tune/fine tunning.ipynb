{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.models import utils\n",
    "\n",
    "from model import MAE\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "model_path = 'D:\\google drive\\MAE Bs PJ\\state_dict\\epoch 3-4200 weight.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_and_split\n",
    "data_folder = 'dataset'\n",
    "test_folder = 'test_set'\n",
    "# extract_and_split(zip_file_path = 'dataset.zip',\n",
    "#          extract_folder = 'dataset',\n",
    "#          test_folder = test_folder,\n",
    "#          data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = torchvision.models.vit_b_16(weights=None)\n",
    "pretrained_model = MAE(vit, 64, 80)\n",
    "pretrained_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_class):\n",
    "        super(ViT, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "                    pretrained_model.backbone,\n",
    "                    nn.Linear(768, num_class),\n",
    "                    nn.Softmax(dim=1)\n",
    "                )\n",
    "        self.out_dim = 80\n",
    "\n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        seq_length = images.shape[1]\n",
    "        images = images.reshape((batch_size, seq_length, self.out_dim))\n",
    "        x = self.backbone(images)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (backbone): Sequential(\n",
       "    (0): MAEBackbone(\n",
       "      (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (encoder): MAEEncoder(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): Sequential(\n",
       "          (encoder_layer_0): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_1): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_2): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_3): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_4): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_5): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_6): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_7): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_8): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_9): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_10): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (encoder_layer_11): EncoderBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (heads): Sequential(\n",
       "        (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "      )\n",
       "      (linear_layer): Linear(in_features=80, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (2): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 32\n",
    "Epochs = 20\n",
    "num_classes = len(os.listdir(r'dataset'))\n",
    "\n",
    "model = ViT(pretrained_model, num_class = num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model = model.to('cuda')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = 'checkpoint.tar'\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "step = checkpoint['step']\n",
    "mean = checkpoint['mean']\n",
    "std = checkpoint['std']\n",
    "total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_data, Node_Dataset, get_time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "data_folder = r'dataset'\n",
    "test_folder = r'test_set'\n",
    "\n",
    "dataset = Node_Dataset(data_folder, mean, std)\n",
    "trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = Node_Dataset(test_folder, mean, std)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuang0jung\u001b[0m (\u001b[33mgethation-rx\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Project-SEKAI\\Project-SEKAI\\MaE fine tune\\wandb\\run-20240526_153222-rzttmyef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gethation-rx/MaE%20fine%20tune/runs/rzttmyef\" target=\"_blank\">batch_size: 32-2</a></strong> to <a href=\"https://wandb.ai/gethation-rx/MaE%20fine%20tune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/gethation-rx/MaE%20fine%20tune\" target=\"_blank\">https://wandb.ai/gethation-rx/MaE%20fine%20tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/gethation-rx/MaE%20fine%20tune/runs/rzttmyef\" target=\"_blank\">https://wandb.ai/gethation-rx/MaE%20fine%20tune/runs/rzttmyef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project='MaE fine tune',\n",
    "    name=f'batch_size: {batch_size}-2',\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"ViT\",\n",
    "        \"dataset\": \"customer\",\n",
    "        \"epochs\": Epochs,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std})\n",
    "config = wandb.config\n",
    "\n",
    "wandb.watch(model)\n",
    "epoch = 0\n",
    "step = 0\n",
    "total_loss = 0\n",
    "total_accuracy = 0\n",
    "model_savepath = f'state_dict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import video_show, train_step, Evaluate, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63a00d600854779b5476e31126de2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss\": 77.97797393798828,\"accuracy\": 61.0%|\"test_loss\": 1.0691322088241577,\"evaluative_accuracy\": 41.18%\n",
      "epoch: 00, time: 23:42:48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb0b2a5ac8048159111a6ccff97a4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss\": 69.24776458740234,\"accuracy\": 73.0%|\"test_loss\": 0.9616696238517761,\"evaluative_accuracy\": 58.82%\n",
      "epoch: 01, time: 23:52:44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b2c6c64fef454db569d0efada20930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss\": 66.07816314697266,\"accuracy\": 76.0%|\"test_loss\": 0.9545019865036011,\"evaluative_accuracy\": 52.94%\n",
      "epoch: 02, time: 24:2:50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe430119ea54fdcb951878ba15f6c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(Epochs):\n",
    "    for i, (batch, target) in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "\n",
    "        loss, accuracy = train_step(model,\n",
    "                    criterion,\n",
    "                    batch,\n",
    "                    target,\n",
    "                    optimizer,\n",
    "                    num_classes=num_classes)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "        with torch.no_grad():\n",
    "            evaluative_loss, evaluative_accuracy = Evaluate(model, criterion, testloader, num_classes)\n",
    "\n",
    "        wandb.log({\"accuracy\": accuracy,\"evaluative_accuracy\": evaluative_accuracy})\n",
    "    total_accuracy = total_accuracy//(i+1)\n",
    "    torch.save(model.state_dict(),os.path.join(model_savepath, f'2epoch-{epoch+1}weight.pt'))\n",
    "    print(f'train_loss\": {total_loss},\"accuracy\": {total_accuracy}%|\"test_loss\": {evaluative_loss},\"evaluative_accuracy\": {evaluative_accuracy}%')\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    print(f'epoch: {epoch:>02}, time: {get_time()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v11-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
